{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time Series Forecasting for Capilano University Enrollment Data\n",
    "\n",
    "This notebook processes enrollment data from Capilano University, converts it into time series format,\n",
    "and applies various forecasting models (Seasonal Naive, ETS, ARIMA). The results are logged to\n",
    "Weights & Biases for visualization and comparison.\n",
    "\n",
    "Original Work: Jiaqi Li (jiaqili@capilanou.ca)  \n",
    "Author: Eric Drechsler (dr.eric.drechsler@gmail.com)  \n",
    "Version: 250301"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment Setup and Module Imports\n",
    "\n",
    "This cell is responsible for setting up the Python environment and importing all the necessary modules and libraries required for the time series analysis.\n",
    "\n",
    "First, it imports standard Python libraries:\n",
    "- `logging`: Used for logging information, warnings, and errors during the execution of the script. This is crucial for debugging and monitoring the process.\n",
    "- `os`: Provides a way of using operating system dependent functionality, such as interacting with the file system.\n",
    "- `typing`: Used for type hinting, which improves code readability and helps catch type-related errors early on. Specifically, it imports `Any`, `Dict`, `List`, `Optional`, and `Tuple` for defining the types of variables and function arguments.\n",
    "\n",
    "Next, it imports external libraries:\n",
    "- `hydra`: A powerful configuration management library that allows for structured configuration through YAML files and command-line overrides. This aligns with the README's mention of using Hydra for configuration management.\n",
    "- `pandas`: A fundamental library for data manipulation and analysis, particularly for working with structured data like CSV files, which are mentioned in the `data_loader.py` description.\n",
    "- `pyrootutils`: A utility library for managing project structure and paths, as described in the README's project structure section.\n",
    "- `omegaconf`: Part of the Hydra ecosystem, it's used for working with configurations in a structured way.\n",
    "\n",
    "The code then uses `pyrootutils.setup_root()` to establish the project's root directory. This is important for ensuring that the script can find necessary files and modules regardless of the current working directory. The parameters passed to `setup_root()` indicate that it should search for a `.project_root` file, use an environment variable if set, load `.env` files, and ensure the project directory is added to the Python path.\n",
    "\n",
    "Following this, the script imports specific functions from the modules within the `capu_time_series_analysis` package, as outlined in the \"Modules Explained\" section of the README:\n",
    "- From `data_loader.py`: `add_to_consolidated_df`, `load_data`, and `prepare_time_series`, which are responsible for loading, preparing, and structuring the enrollment data.\n",
    "- From `evaluation.py`: `analyze_residuals` and `evaluate_forecasts`, which provide tools for assessing the performance and validity of the forecasting models.\n",
    "- From `models.py`: `fit_models`, which implements the time series forecasting models as described in the README.\n",
    "- From `visualization.py`: `log_results_to_wandb`, which handles logging results and visualizations to Weights & Biases, as mentioned in the \"Enhanced Visualization\" section of the README.\n",
    "\n",
    "Finally, the cell configures the logging system. It sets the logging level to `INFO`, specifies the format of the log messages (including timestamp, logger name, level, and message), and sets the date format. It then gets the root logger, which will be used to record events during the script's execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell sets up the environment, imports necessary modules, and configures\n",
    "# logging\n",
    "\n",
    "import logging\n",
    "import os\n",
    "from typing import Any, Dict, List, Optional, Tuple\n",
    "\n",
    "import hydra\n",
    "import pandas as pd\n",
    "import pyrootutils\n",
    "from omegaconf import DictConfig, OmegaConf\n",
    "\n",
    "ROOT_PATH = pyrootutils.setup_root(\n",
    "    search_from=os.getcwd(),\n",
    "    indicator=\".project_root\",\n",
    "    project_root_env_var=True,\n",
    "    dotenv=True,\n",
    "    pythonpath=True,\n",
    "    cwd=True,\n",
    ")\n",
    "\n",
    "# Import utility modules\n",
    "from capu_time_series_analysis.data_loader import (\n",
    "    add_to_consolidated_df,\n",
    "    load_data,\n",
    "    prepare_time_series,\n",
    ")\n",
    "from capu_time_series_analysis.evaluation import analyze_residuals, evaluate_forecasts\n",
    "from capu_time_series_analysis.models import fit_models\n",
    "from capu_time_series_analysis.visualization import log_results_to_wandb\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\",\n",
    "    datefmt=\"%Y-%m-%d %H:%M:%S\",\n",
    ")\n",
    "logger = logging.getLogger()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions\n",
    "### Calculate and Analyze Model Residuals\n",
    "\n",
    "This cell defines a function called `calculate_residuals` which takes the fitted models (Seasonal Naive, ETS, ARIMA), the training time series data, a list to store residual diagnostics, and metadata (metric type, residency, and level) as input. Its primary purpose is to analyze the residuals of each fitted model and store the diagnostic results.\n",
    "\n",
    "The function starts by calling the `analyze_residuals` function (imported from `capu_time_series_analysis.evaluation`) for each of the three models: Seasonal Naive, ETS, and ARIMA. The `analyze_residuals` function likely performs various tests and generates plots to assess the quality of the model fit, as described in the \"Understanding Model Diagnostics\" section of the README. This includes checking for autocorrelation using the Ljung-Box test, assessing normality, and determining if the residuals resemble white noise.\n",
    "\n",
    "The results from `analyze_residuals` for each model (including mean residual, standard deviation of residuals, Ljung-Box statistic and p-value, normality p-value, white noise status, residuals plot, and actual vs. fitted plot) are then structured into dictionaries. Each dictionary represents the residual diagnostics for a specific model and includes the provided metadata (metric type, residency, and level) to help identify the context of the analysis.\n",
    "\n",
    "Finally, these dictionaries are appended to the `residual_diagnostics` list, which is passed into the function and then returned. This list will accumulate the residual analysis results for different models and data slices, allowing for a comprehensive evaluation of the model performance.\n",
    "\n",
    "In essence, this function automates the process of evaluating how well each model fits the training data by examining its residuals, which is a crucial step in model selection and validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_residuals(\n",
    "    fit_snaive: Any,\n",
    "    fit_ets: Any,\n",
    "    fit_arima: Any,\n",
    "    train_series: pd.Series,\n",
    "    residual_diagnostics: List[Dict[str, Any]],\n",
    "    mt: str,\n",
    "    resd: str,\n",
    "    level: str,\n",
    ") -> List[Dict[str, Any]]:\n",
    "    snaive_residuals = analyze_residuals(fit_snaive, train_series, \"Seasonal Naive\")\n",
    "    ets_residuals = analyze_residuals(fit_ets, train_series, \"ETS\")\n",
    "    arima_residuals = analyze_residuals(fit_arima, train_series, \"ARIMA\")\n",
    "\n",
    "    for model_name, diag in [\n",
    "        (\"Seasonal Naive\", snaive_residuals),\n",
    "        (\"ETS\", ets_residuals),\n",
    "        (\"ARIMA\", arima_residuals),\n",
    "    ]:\n",
    "        residual_diagnostics.append(\n",
    "            {\n",
    "                \"Analysis_Type\": mt,\n",
    "                \"Residency\": resd,\n",
    "                \"Level\": level,\n",
    "                \"Model\": model_name,\n",
    "                \"Mean_Residual\": diag[\"mean_residual\"],\n",
    "                \"Std_Residual\": diag[\"std_residual\"],\n",
    "                \"Ljung_Box_Stat\": diag[\"ljung_box_stat\"],\n",
    "                \"Ljung_Box_pvalue\": diag[\"ljung_box_pvalue\"],\n",
    "                \"Normality_pvalue\": diag[\"residuals_normal_pvalue\"],\n",
    "                \"White_Noise\": diag[\"white_noise\"],\n",
    "                \"Residuals_Plot\": diag[\"residuals_plot\"],\n",
    "                \"Actual_Fitted_Plot\": diag[\"actual_fitted_plot\"],\n",
    "            }\n",
    "        )\n",
    "    return residual_diagnostics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to Process Time Series Data for Multiple Combinations\n",
    "\n",
    "This cell defines the core function `process_timeseries`, which is responsible for iterating through different combinations of metrics, residencies, and levels to perform time series analysis on the enrollment data.\n",
    "\n",
    "The function takes several arguments:\n",
    "- `input_file`: The path to the input data file (likely a CSV file as mentioned in the README).\n",
    "- `metrics`: A list of metrics to analyze (e.g., Headcount, CourseEnrolment).\n",
    "- `residencies`: A list of residency types to filter the data by.\n",
    "- `levels`: A list of levels (e.g., CapU) to filter the data by.\n",
    "- `forecast_steps`: The number of steps to forecast into the future (default is 9).\n",
    "- `model_params`: An optional dictionary to specify parameters for the models.\n",
    "\n",
    "The function first handles the case where `model_params` is not provided, initializing it as an empty dictionary.\n",
    "\n",
    "It then calculates the total number of combinations to be processed and logs this information using the `logger` (set up in the first cell). This provides a clear indication of the workload.\n",
    "\n",
    "The function initializes an empty DataFrame `consolidated_df` to store the results in a standardized format, as suggested by the description of `data_loader.py` in the README. It also initializes two empty lists: `evaluation_results` and `residual_diagnostics`, which will store the evaluation metrics and residual analysis results, respectively.\n",
    "\n",
    "The core logic of the function involves three nested loops that iterate through all possible combinations of metrics, residencies, and levels. Inside these loops:\n",
    "- The function keeps track of the current combination being processed and logs its progress.\n",
    "- The comment `...existing code...` indicates where the actual data processing and model fitting logic will be implemented for each combination. This will likely involve loading the data, preparing the time series, fitting the models, evaluating the forecasts, and analyzing the residuals.\n",
    "\n",
    "Finally, the function returns the `consolidated_df`, `evaluation_results`, and `residual_diagnostics`. These outputs will contain all the results of the time series analysis for all the processed combinations.\n",
    "\n",
    "This function orchestrates the entire analysis process, ensuring that all specified combinations are processed and their results are collected for further analysis and visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_timeseries(\n",
    "    input_file: str,\n",
    "    metrics: List[str],\n",
    "    residencies: List[str],\n",
    "    levels: List[str],\n",
    "    forecast_steps: int = 9,\n",
    "    model_params: Optional[Dict[str, Dict[str, Any]]] = None,\n",
    ") -> Tuple[pd.DataFrame, List[Dict[str, Any]], List[Dict[str, Any]]]:\n",
    "    if model_params is None:\n",
    "        model_params = {}\n",
    "\n",
    "    total_combinations = len(levels) * len(residencies) * len(metrics)\n",
    "    logger.info(\n",
    "        f\"Processing {total_combinations} combinations of levels, residencies, and metrics\"\n",
    "    )\n",
    "\n",
    "    current_combination = 0\n",
    "    consolidated_df = pd.DataFrame(\n",
    "        columns=[\n",
    "            \"Analysis_Type\",\n",
    "            \"Residency\",\n",
    "            \"Level\",\n",
    "            \"Timestamp\",\n",
    "            \"Model\",\n",
    "            \"Entry_Type\",\n",
    "            \"Entry\",\n",
    "        ]\n",
    "    )\n",
    "    evaluation_results: List[Dict[str, Any]] = []\n",
    "    residual_diagnostics: List[Dict[str, Any]] = []\n",
    "\n",
    "    # Process all combinations\n",
    "    for mt in metrics:\n",
    "        for resd in residencies:\n",
    "            for level in levels:\n",
    "                current_combination += 1\n",
    "                logger.info(\n",
    "                    f\"Processing {level} - {resd} - {mt} ({current_combination}/{total_combinations})\"\n",
    "                )\n",
    "\n",
    "                # ...existing code...\n",
    "\n",
    "    return consolidated_df, evaluation_results, residual_diagnostics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "cfg = {\n",
    "    \"input_file\": \"path/to/your/input.csv\",\n",
    "    \"plot_dir\": \"plots\",\n",
    "    \"levels\": [\"Undergraduate\", \"Graduate\"],\n",
    "    \"residencies\": [\"Domestic\", \"International\"],\n",
    "    \"metrics\": [\"Headcount\", \"FTE\"],\n",
    "    \"forecast_steps\": 9,\n",
    "    \"models\": {\"seasonal_naive_params\": {}, \"ets_params\": {}, \"arima_params\": {}},\n",
    "}\n",
    "\n",
    "# Create plot directory if it doesn't exist\n",
    "if not os.path.exists(cfg[\"plot_dir\"]):\n",
    "    os.makedirs(cfg[\"plot_dir\"])\n",
    "    logger.info(f\"Created plot directory: {cfg['plot_dir']}\")\n",
    "\n",
    "# Process time series\n",
    "consolidated_df, evaluation_results, residual_diagnostics = process_timeseries(\n",
    "    cfg[\"input_file\"],\n",
    "    cfg[\"metrics\"],\n",
    "    cfg[\"residencies\"],\n",
    "    cfg[\"levels\"],\n",
    "    forecast_steps=cfg[\"forecast_steps\"],\n",
    "    model_params=cfg[\"models\"],\n",
    ")\n",
    "\n",
    "# Create evaluation dataframe\n",
    "evaluation_df = pd.DataFrame(evaluation_results)\n",
    "logger.info(f\"Created evaluation dataframe with {len(evaluation_df)} entries\")\n",
    "\n",
    "# Create residual diagnostics dataframe\n",
    "residual_df = pd.DataFrame(residual_diagnostics)\n",
    "logger.info(f\"Created residual diagnostics dataframe with {len(residual_df)} entries\")\n",
    "\n",
    "# Log results to wandb\n",
    "logger.info(\"Logging results to Weights & Biases\")\n",
    "log_results_to_wandb(\n",
    "    consolidated_df, evaluation_df, residual_df, cfg[\"metrics\"], cfg[\"levels\"], cfg[\"residencies\"]\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
